# PolyglotAI - å®Ÿè£…ã‚¬ã‚¤ãƒ‰

ã“ã®ã‚¬ã‚¤ãƒ‰ã¯PolyglotAIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®é–‹ç™ºè€…å‘ã‘ã®æŠ€è¡“çš„ãªå®Ÿè£…ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚å„æ©Ÿèƒ½ã®å®Ÿè£…æ–¹æ³•ã¨æ³¨æ„ç‚¹ã‚’èª¬æ˜ã—ã¾ã™ã€‚

## ç›®æ¬¡

1. [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—](#1-ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—)
2. [éŸ³å£°èªè­˜ã®å®Ÿè£…](#2-éŸ³å£°èªè­˜ã®å®Ÿè£…)
3. [GPT-4ã‚’ä½¿ç”¨ã—ãŸæ–‡æ³•è¨‚æ­£](#3-gpt-4ã‚’ä½¿ç”¨ã—ãŸæ–‡æ³•è¨‚æ­£)
4. [Text-to-Speechã®å®Ÿè£…](#4-text-to-speechã®å®Ÿè£…) 
5. [UIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ](#5-uiã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ)
6. [ç’°å¢ƒå¤‰æ•°ã®è¨­å®š](#6-ç’°å¢ƒå¤‰æ•°ã®è¨­å®š)
7. [ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¬ã‚¤ãƒ‰](#7-ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¬ã‚¤ãƒ‰)

## 1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 1.1 åˆæœŸã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# Next.jsãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ
npx create-next-app polyglot-ai --typescript --tailwind --eslint --app --src-dir

# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
cd polyglot-ai
npm install framer-motion @clerk/nextjs @supabase/supabase-js @google-cloud/speech @google-cloud/text-to-speech openai

# Shadcn UIã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
npx shadcn@latest init
npx shadcn@latest add button card dialog input sonner textarea progress sheet avatar dropdown-menu
```

### 1.2 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
polyglot-ai/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ correct/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ route.ts  # GPT-4ã«ã‚ˆã‚‹æ–‡æ³•è¨‚æ­£API
â”‚   â”‚   â”‚   â””â”€â”€ speak/
â”‚   â”‚   â”‚       â””â”€â”€ route.ts  # Text-to-Speech API
â”‚   â”‚   â”œâ”€â”€ practice/
â”‚   â”‚   â”‚   â””â”€â”€ page.tsx      # ç·´ç¿’ãƒšãƒ¼ã‚¸
â”‚   â”‚   â”œâ”€â”€ globals.css
â”‚   â”‚   â”œâ”€â”€ layout.tsx
â”‚   â”‚   â””â”€â”€ page.tsx          # ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ui/               # Shadcn UIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
â”‚   â”‚   â””â”€â”€ custom/           # ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
â”‚   â””â”€â”€ lib/
â”‚       â””â”€â”€ utils.ts          # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
â”œâ”€â”€ docs/                     # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä»•æ§˜ã¨ã‚¬ã‚¤ãƒ‰
â”œâ”€â”€ public/
â””â”€â”€ ...è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
```

## 2. éŸ³å£°èªè­˜ã®å®Ÿè£…

PolyglotAIã¯ãƒ–ãƒ©ã‚¦ã‚¶ã®`Web Speech API`ã‚’ä½¿ç”¨ã—ã¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ã§éŸ³å£°èªè­˜ã‚’è¡Œã„ã¾ã™ã€‚

### 2.1 Web Speech APIã®åŸºæœ¬å®Ÿè£…

```typescript
// src/app/practice/page.tsx ã®ä¸€éƒ¨
"use client";

import { useState, useEffect, useRef } from "react";

export default function PracticePage() {
  const [isListening, setIsListening] = useState(false);
  const [transcript, setTranscript] = useState<string>("");
  const [selectedLanguage, setSelectedLanguage] = useState("en-US");
  const recognitionRef = useRef<any>(null);

  // éŸ³å£°èªè­˜ã®åˆæœŸåŒ–
  useEffect(() => {
    if (typeof window !== "undefined" && 'webkitSpeechRecognition' in window) {
      // @ts-ignore - webkitSpeechRecognitionã¯Typesã«å«ã¾ã‚Œã¦ã„ãªã„
      const SpeechRecognition = window.webkitSpeechRecognition || window.SpeechRecognition;
      recognitionRef.current = new SpeechRecognition();
      recognitionRef.current.continuous = true;
      recognitionRef.current.interimResults = true;
      recognitionRef.current.lang = selectedLanguage;

      recognitionRef.current.onresult = (event: any) => {
        const current = event.resultIndex;
        const result = event.results[current];
        const transcriptText = result[0].transcript;
        setTranscript(transcriptText);
        
        // ç™ºè©±ãŒä¸€æ™‚åœæ­¢ã—ãŸã¨ãã«GPTã«ã‚ˆã‚‹è¨‚æ­£å‡¦ç†ã‚’å®Ÿè¡Œ
        if (result.isFinal) {
          processTextCorrection(transcriptText);
        }
      };

      recognitionRef.current.onerror = (event: any) => {
        console.error("Speech recognition error", event.error);
        // ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
      };
    }

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
    };
  }, [selectedLanguage]);

  const toggleListening = () => {
    if (isListening) {
      recognitionRef.current?.stop();
      setIsListening(false);
    } else {
      recognitionRef.current?.start();
      setIsListening(true);
      setTranscript("");
    }
  };

  // ...ãã®ä»–ã®ã‚³ãƒ¼ãƒ‰
}
```

### 2.2 ã‚µãƒãƒ¼ãƒˆã•ã‚Œã‚‹è¨€èª

ä¸»è¦ãªè¨€èªã‚³ãƒ¼ãƒ‰ã®ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ï¼š

```typescript
const languageOptions = [
  { value: "en-US", label: "English (US)" },
  { value: "en-GB", label: "English (UK)" },
  { value: "fr-FR", label: "French" },
  { value: "de-DE", label: "German" },
  { value: "es-ES", label: "Spanish" },
  { value: "ja-JP", label: "Japanese" }
];
```

## 3. GPT-4ã‚’ä½¿ç”¨ã—ãŸæ–‡æ³•è¨‚æ­£

### 3.1 APIå®Ÿè£…

```typescript
// src/app/api/correct/route.ts
import { NextResponse } from 'next/server';
import OpenAI from 'openai';

// OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

export async function POST(req: Request) {
  try {
    const { text, language } = await req.json();

    if (!text || text.trim() === '') {
      return NextResponse.json(
        { error: 'No text provided' },
        { status: 400 }
      );
    }

    // GPT-4ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
    const prompt = generatePrompt(text, language || 'en-US');

    // OpenAI APIã®å‘¼ã³å‡ºã—
    const response = await openai.chat.completions.create({
      model: 'gpt-4-turbo',
      messages: [
        {
          role: 'system',
          content: `
          ã‚ãªãŸã¯è¨€èªæ•™å¸«ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚æä¾›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®æ–‡æ³•ã‚¨ãƒ©ãƒ¼ã€
          èªå½™ã®å•é¡Œã€ä¸è‡ªç„¶ãªè¡¨ç¾ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚
          ä¿®æ­£ã¨èª¬æ˜ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
          
          ä»¥ä¸‹ã®JSONå½¢å¼ã§ã®ã¿å¿œç­”ã—ã¦ãã ã•ã„:
          [
            {
              "text": "ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆ",
              "isCorrect": false,
              "correction": "ä¿®æ­£ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆ",
              "explanation": "ã‚¨ãƒ©ãƒ¼ã®ç°¡å˜ãªèª¬æ˜"
            },
            {
              "text": "ã‚¨ãƒ©ãƒ¼ã®ãªã„å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆ",
              "isCorrect": true
            }
          ]
          
          ã‚¨ãƒ©ãƒ¼ãŒãªã„å ´åˆã¯ç©ºã®é…åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
          `
        },
        {
          role: 'user',
          content: prompt
        }
      ],
      response_format: { type: 'json_object' }
    });

    // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è§£æ
    const content = response.choices[0]?.message?.content || '';
    const correctionData = JSON.parse(content);
    
    return NextResponse.json(correctionData);
  } catch (error) {
    console.error('Error correcting speech:', error);
    return NextResponse.json(
      { error: 'Failed to process correction' },
      { status: 500 }
    );
  }
}

// è¨€èªã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç”Ÿæˆ
function generatePrompt(text: string, language: string): string {
  const languageMap: Record<string, string> = {
    'en-US': 'American English',
    'en-GB': 'British English',
    'fr-FR': 'French',
    'de-DE': 'German',
    'es-ES': 'Spanish',
  };

  const languageName = languageMap[language] || 'English';

  return `
  ä»¥ä¸‹ã¯${languageName}ã®è©±ã—è¨€è‘‰ã§ã™ã€‚
  æ–‡æ³•ã‚¨ãƒ©ãƒ¼ã€èªå½™ã®èª¤ç”¨ã€ã¾ãŸã¯ä¸è‡ªç„¶ãªè¡¨ç¾ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚
  å„å•é¡Œã«ã¤ã„ã¦ä¿®æ­£ã¨ç°¡å˜ãªèª¬æ˜ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
  
  ãƒ†ã‚­ã‚¹ãƒˆ: "${text}"
  `;
}
```

### 3.2 ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§ã®çµ±åˆ

```typescript
// src/app/practice/page.tsx ã®ä¸€éƒ¨
const processTextCorrection = async (text: string) => {
  if (!text.trim()) return;
  
  setProcessingCorrection(true);
  
  try {
    const response = await fetch('/api/correct', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        text,
        language: selectedLanguage
      }),
    });

    if (!response.ok) {
      throw new Error('API request failed');
    }

    const data = await response.json();
    setCorrectedSegments(data);
    
    // ä¿®æ­£ã•ã‚ŒãŸæ–‡ã‚’éŸ³å£°ã§å†ç”Ÿ
    if (data[0]?.correction) {
      playCorrectedAudio(data[0].correction);
    }
  } catch (error) {
    console.error("Error processing correction:", error);
    toast.error("ä¿®æ­£ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚");
  } finally {
    setProcessingCorrection(false);
  }
};
```

## 4. Text-to-Speechã®å®Ÿè£…

### 4.1 APIå®Ÿè£…

```typescript
// src/app/api/speak/route.ts
import { NextResponse } from 'next/server';
import { TextToSpeechClient } from '@google-cloud/text-to-speech';

// Text-to-Speech ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
const textToSpeechClient = new TextToSpeechClient({
  credentials: JSON.parse(process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON || '{}'),
});

export async function POST(req: Request) {
  try {
    const { text, language } = await req.json();

    if (!text || text.trim() === '') {
      return NextResponse.json(
        { error: 'No text provided' },
        { status: 400 }
      );
    }

    // è¨€èªã‚³ãƒ¼ãƒ‰ã¨éŸ³å£°åã®è¨­å®š
    const languageCode = getLanguageCode(language || 'en-US');
    const voiceName = getVoiceName(language || 'en-US');

    // ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®è¨­å®š
    const request = {
      input: { text },
      voice: {
        languageCode,
        name: voiceName,
        ssmlGender: 'NEUTRAL',
      },
      audioConfig: {
        audioEncoding: 'MP3',
        pitch: 0,
        speakingRate: 1,
      },
    };

    // Google Cloud Text-to-Speech APIã®å‘¼ã³å‡ºã—
    const [response] = await textToSpeechClient.synthesizeSpeech(request);
    const audioContent = response.audioContent;

    if (!audioContent) {
      throw new Error('Failed to generate audio content');
    }

    // éŸ³å£°ã‚’base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦è¿”ã™
    const audioBase64 = Buffer.from(audioContent).toString('base64');
    return NextResponse.json({ audioContent: audioBase64 });
  } catch (error) {
    console.error('Error generating speech:', error);
    return NextResponse.json(
      { error: 'Failed to generate speech' },
      { status: 500 }
    );
  }
}

// è¨€èªã‚³ãƒ¼ãƒ‰ã®å–å¾—
function getLanguageCode(language: string): string {
  const languageCodes: Record<string, string> = {
    'en-US': 'en-US',
    'en-GB': 'en-GB',
    'fr-FR': 'fr-FR',
    'de-DE': 'de-DE',
    'es-ES': 'es-ES',
  };

  return languageCodes[language] || 'en-US';
}

// éŸ³å£°åã®å–å¾—
function getVoiceName(language: string): string {
  const voiceNames: Record<string, string> = {
    'en-US': 'en-US-Neural2-C',
    'en-GB': 'en-GB-Neural2-B',
    'fr-FR': 'fr-FR-Neural2-A',
    'de-DE': 'de-DE-Neural2-B',
    'es-ES': 'es-ES-Neural2-C',
  };

  return voiceNames[language] || 'en-US-Neural2-C';
}
```

### 4.2 ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§ã®çµ±åˆ

```typescript
// src/app/practice/page.tsx ã®ä¸€éƒ¨
const playCorrectedAudio = async (text: string) => {
  try {
    const response = await fetch('/api/speak', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        text,
        language: selectedLanguage
      }),
    });

    if (!response.ok) {
      throw new Error('API request failed');
    }

    const data = await response.json();
    
    // Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦å†ç”Ÿ
    if (data.audioContent) {
      const audioSrc = `data:audio/mp3;base64,${data.audioContent}`;
      
      if (audioRef.current) {
        audioRef.current.src = audioSrc;
        audioRef.current.play().catch(e => {
          console.error("Error playing audio:", e);
          toast.error("éŸ³å£°ã®å†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸã€‚");
        });
      }
    }
  } catch (error) {
    console.error("Error playing corrected audio:", error);
    toast.error("éŸ³å£°ã®å†ç”Ÿä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚");
  }
};
```

## 5. UIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

### 5.1 è¨‚æ­£è¡¨ç¤ºã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

```tsx
// src/app/practice/page.tsx ã®ä¸€éƒ¨
// è¨‚æ­£ã¨èª¬æ˜ã®è¡¨ç¤º
<AnimatePresence>
  {correctedSegments.length > 0 && (
    <motion.div
      initial={{ opacity: 0, y: 10 }}
      animate={{ opacity: 1, y: 0 }}
      exit={{ opacity: 0 }}
      className="mt-4"
    >
      <h3 className="font-semibold mb-2 text-blue-900">ä¿®æ­£ã¨èª¬æ˜:</h3>
      <div className="space-y-4">
        {correctedSegments.map((segment, idx) => (
          <motion.div 
            key={idx}
            initial={{ opacity: 0, x: -5 }}
            animate={{ opacity: 1, x: 0 }}
            transition={{ delay: idx * 0.1 }}
            className="bg-white p-4 rounded-md border-l-4 border-l-amber-500"
          >
            <div className="flex flex-col space-y-2">
              <div>
                <span className="font-medium">å…ƒã®æ–‡: </span>
                <span className="text-red-600">{segment.text}</span>
              </div>
              <div>
                <span className="font-medium">ä¿®æ­£: </span>
                <span className="text-green-600">{segment.correction}</span>
              </div>
              {segment.explanation && (
                <div>
                  <span className="font-medium">èª¬æ˜: </span>
                  <span>{segment.explanation}</span>
                </div>
              )}
              <Button 
                onClick={() => segment.correction && playCorrectedAudio(segment.correction)}
                variant="outline" 
                className="self-start mt-2"
                size="sm"
              >
                <span className="mr-2">ğŸ”Š</span> ä¿®æ­£ã‚’èã
              </Button>
            </div>
          </motion.div>
        ))}
      </div>
    </motion.div>
  )}
</AnimatePresence>
```

### 5.2 éŒ²éŸ³ãƒœã‚¿ãƒ³ã¨å­—å¹•è¡¨ç¤º

```tsx
// src/app/practice/page.tsx ã®ä¸€éƒ¨
// éŒ²éŸ³ãƒœã‚¿ãƒ³
<div className="flex justify-center mb-4">
  <Button 
    onClick={toggleListening}
    className={`px-6 py-5 text-lg ${isListening ? 'bg-red-600 hover:bg-red-700' : 'bg-blue-600 hover:bg-blue-700'}`}
  >
    {isListening ? 'éŒ²éŸ³ã‚’åœæ­¢' : 'éŒ²éŸ³ã‚’é–‹å§‹'}
  </Button>
</div>

// å­—å¹•è¡¨ç¤º
<div className="bg-white p-4 rounded-md min-h-20 border border-blue-100">
  {transcript ? (
    <p className="text-lg">{transcript}</p>
  ) : (
    <p className="text-gray-400 text-center">ã€ŒéŒ²éŸ³ã‚’é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦è©±ã—ã¦ãã ã•ã„...</p>
  )}
</div>

// å‡¦ç†ä¸­ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼
{processingCorrection && (
  <div className="text-center">
    <p className="mb-2 text-blue-800">ä¿®æ­£ã‚’å‡¦ç†ä¸­...</p>
    <Progress value={66} className="w-full h-2" />
  </div>
)}
```

## 6. ç’°å¢ƒå¤‰æ•°ã®è¨­å®š

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆã«`.env.local`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¾ã™ï¼š

```
# OpenAI API
OPENAI_API_KEY=sk_...

# Google Cloud
GOOGLE_APPLICATION_CREDENTIALS_JSON={"type":"service_account",...}

# Clerk (èªè¨¼)
NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=pk_...
CLERK_SECRET_KEY=sk_...

# Supabase (ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹)
SUPABASE_URL=https://...
SUPABASE_ANON_KEY=...
```

## 7. ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¬ã‚¤ãƒ‰

### 7.1 Vercelã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤

1. [Vercel](https://vercel.com/)ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ä½œæˆ
2. GitHubãƒªãƒã‚¸ãƒˆãƒªã¨é€£æº
3. ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
4. ãƒ‡ãƒ—ãƒ­ã‚¤ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯

```bash
# ãƒ­ãƒ¼ã‚«ãƒ«ã§ã®Vercelãƒ‡ãƒ—ãƒ­ã‚¤ (Vercel CLIãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å ´åˆ)
vercel
```

### 7.2 ãã®ä»–ã®ãƒ‡ãƒ—ãƒ­ã‚¤å…ˆ

- **Netlify**: åŒæ§˜ã«GitHubãƒªãƒã‚¸ãƒˆãƒªã¨é€£æºã—ã€ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
- **AWS Amplify**: AWSç®¡ç†ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‹ã‚‰ãƒ‡ãƒ—ãƒ­ã‚¤è¨­å®š
- **DigitalOcean**: App Platformã‚’åˆ©ç”¨ã—ãŸãƒ‡ãƒ—ãƒ­ã‚¤

### 7.3 ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°

```bash
# ãƒ“ãƒ«ãƒ‰
npm run build

# èµ·å‹•
npm start
```

## 8. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨æœ€é©åŒ–ã®ãƒ’ãƒ³ãƒˆ

- APIã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’å®Ÿè£…ã—ã€ã‚³ã‚¹ãƒˆç®¡ç†
- ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã«ã‚ˆã‚‹å¿œç­”æ€§ã®å‘ä¸Š
- ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ˆã‚‹APIå‘¼ã³å‡ºã—ã®å‰Šæ¸›
- WebSocketã‚’æ´»ç”¨ã—ãŸãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ï¼ˆé€²æ—ãƒ‡ãƒ¼ã‚¿ãªã©ï¼‰

## 9. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

- **éŸ³å£°èªè­˜ãŒå‹•ä½œã—ãªã„**: ãƒ–ãƒ©ã‚¦ã‚¶ãŒWeb Speech APIã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã‹ç¢ºèª
- **APIã‚¨ãƒ©ãƒ¼**: ç’°å¢ƒå¤‰æ•°ã¨APIã‚­ãƒ¼ã®è¨­å®šã‚’ç¢ºèª
- **ãƒ‡ãƒ—ãƒ­ã‚¤å•é¡Œ**: ç’°å¢ƒå¤‰æ•°ã¨ãƒ“ãƒ«ãƒ‰è¨­å®šã‚’ç¢ºèª 